{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "import numpy as np\n",
    "from panel import state\n",
    "\n",
    "class SawWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "        \n",
    "        self.target_location = np.array([20,0, 0,0]) #dmax= 100, thetamax = 90, ddmax = 20, dthetamax = 20\n",
    "        self.actions = np.array([ [1,1], [1,0], [1,-1],[0,1], [0,0],[0,-1]])\n",
    "        self.max_values = np.array([23,20,5,5,len(self.actions)])\n",
    "        self.angle_scale = .3\n",
    "        self.observation_space = np.array([self.max_values[0],self.max_values[1], self.max_values[2],self.max_values[3]])#d, theta, dd, dtheta, actins\n",
    "        self.states = self.max_values.copy()\n",
    "        self.states[0:4] = self.states[0:4]*2+1\n",
    "        \n",
    "        \n",
    "\n",
    "   \n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    def _get_obs(self):\n",
    "        return self.observation_space\n",
    "    \n",
    "    def _get_info(self):\n",
    "        return -1#{\"distance\": np.linalg.norm(self._agent_location - self._target_location, ord=1)}\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        #self.close()\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Choose the agent's location uniformly at random\n",
    "        \n",
    "        self.observation_space = np.array([self.max_values[0],self.max_values[1], self.max_values[2],self.max_values[3]])#d, theta, dd, dtheta, actins\n",
    "\n",
    "        # We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        terminated = False\n",
    "        reward = 0\n",
    "        \n",
    "        self.observation_space = self.move_saw(self.observation_space, action)\n",
    "\n",
    "        terminated, reward = self.find_terminated(self.observation_space)\n",
    "\n",
    "\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "    \n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        pass\n",
    "        \n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "    \n",
    "    def move_saw(self, state, action):\n",
    "\n",
    "        dd = int(state[2] + self.angle_scale*(state[1]-self.max_values[1])* action[0]) #dd\n",
    "        dtheta = state[3] + action[1] #dtheta\n",
    "\n",
    "\n",
    "        if dd > self.max_values[2]:\n",
    "            state[2] = self.max_values[2]\n",
    "        elif dd < -self.max_values[2]:\n",
    "            state[2] = self.max_values[2]\n",
    "        else:\n",
    "            state[2] = dd\n",
    "\n",
    "        if dtheta > self.max_values[3]:\n",
    "            state[3] = self.max_values[3]\n",
    "        elif dtheta < -self.max_values[3]:\n",
    "            state[3] = self.max_values[3]\n",
    "        else:\n",
    "            state[3] = dtheta\n",
    "\n",
    "        state[0] = state[0] + state[2]\n",
    "        state[1] = state[1] + state[3]\n",
    "        return state\n",
    "\n",
    "    def find_terminated(self, state):\n",
    "        reward = -1\n",
    "        terminated = False\n",
    "        \n",
    "        if state[0] <= 0 or state[0] >= self.states[0]:\n",
    "            terminated = True\n",
    "        elif state[1] <= 0 or state[1] >= self.states[1]:\n",
    "            terminated = True\n",
    "        elif np.array_equal(state, self.target_location):\n",
    "            terminated = True\n",
    "            reward = 0\n",
    "\n",
    "        return terminated, reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval:  160497.5\n",
      "eval:  135860.72222222222\n",
      "eval:  113492.62962962959\n",
      "eval:  93017.10956790124\n",
      "eval:  74405.30979938273\n",
      "eval:  57640.06155692727\n",
      "eval:  42708.26626443185\n",
      "eval:  30947.63545655672\n",
      "eval:  22983.595193484685\n",
      "eval:  17190.689961806074\n",
      "eval:  12838.79501313814\n",
      "eval:  9565.395132464902\n",
      "eval:  7107.000653606765\n",
      "eval:  5268.748667399196\n",
      "eval:  3902.2700682836653\n",
      "eval:  2891.121252523325\n",
      "eval:  2145.265993194608\n",
      "eval:  1596.3095020892808\n",
      "eval:  1192.794613281912\n",
      "eval:  896.2849948305932\n",
      "eval:  678.2331069305284\n",
      "eval:  517.5488656287743\n",
      "eval:  398.7267659235258\n",
      "eval:  310.41952578326016\n",
      "eval:  244.35743543771932\n",
      "eval:  194.5349295997668\n",
      "eval:  156.6023484430215\n",
      "eval:  127.41452717704368\n",
      "eval:  104.69813708682412\n",
      "eval:  86.80833120800328\n",
      "eval:  72.55192310050899\n",
      "eval:  61.05970805028184\n",
      "eval:  51.694755072690505\n",
      "eval:  43.98681670763913\n",
      "eval:  37.58556062714502\n",
      "eval:  32.22727887436794\n",
      "eval:  27.711197414495754\n",
      "eval:  23.882597454261237\n",
      "eval:  20.62075764128296\n",
      "eval:  17.830304278242373\n",
      "eval:  15.434971424768152\n",
      "eval:  13.37306792728022\n",
      "eval:  11.594157109097342\n",
      "eval:  10.056601683178641\n",
      "eval:  8.725729409836893\n",
      "eval:  7.572447084514471\n",
      "eval:  6.572180852524172\n",
      "eval:  5.704056138796293\n",
      "eval:  4.95025523057306\n",
      "eval:  4.295507957124756\n",
      "eval:  3.726683195961898\n",
      "eval:  3.2324576445988216\n",
      "eval:  2.803044503931368\n",
      "eval:  2.429969167727677\n",
      "eval:  2.105882223041939\n",
      "eval:  1.8244023990303802\n",
      "eval:  1.5799838106635582\n",
      "eval:  1.3678031066979617\n",
      "eval:  1.1836630730755522\n",
      "eval:  1.023909952389638\n",
      "eval:  0.8853622799182104\n",
      "eval:  0.7652494520395547\n",
      "eval:  0.6611585658364614\n",
      "eval:  0.5709883224993881\n",
      "eval:  0.4929089888381888\n",
      "eval:  0.4253275732663726\n",
      "eval:  0.3668575037605337\n",
      "eval:  0.3162922027247985\n",
      "eval:  0.27258204249854945\n",
      "eval:  0.23481423873854212\n",
      "eval:  0.20219530104461225\n",
      "eval:  0.1740357118936693\n",
      "eval:  0.14973654942824854\n",
      "eval:  0.12877780726028387\n",
      "eval:  0.11070819651653796\n",
      "eval:  0.09513624329775161\n",
      "eval:  0.08172251859363655\n",
      "eval:  0.07017285816683971\n",
      "eval:  0.06023244826769325\n",
      "eval:  0.05168066819915218\n",
      "eval:  0.04432659476253109\n",
      "eval:  0.03800508513534684\n",
      "eval:  0.03257336535831579\n",
      "eval:  0.027908060446354854\n",
      "eval:  0.023902610304115113\n",
      "eval:  0.0204650223739955\n",
      "eval:  0.017515918201934655\n",
      "eval:  0.014986836249170743\n",
      "eval:  0.01281875832561008\n",
      "eval:  0.01096083064449549\n",
      "eval:  0.009369254552437756\n",
      "eval:  0.00800632485474062\n",
      "eval:  0.006839596603705811\n",
      "eval:  0.005841163571035812\n",
      "eval:  0.0049870337544248855\n",
      "eval:  0.004256589170464009\n",
      "eval:  0.003632118825860564\n",
      "eval:  0.0030984151262234416\n",
      "eval:  0.0026424253534971243\n",
      "eval:  0.002252950774908058\n",
      "iter:  325109.3999999998\n",
      "eval:  2287356.4098006785\n",
      "eval:  9064146.539244128\n",
      "eval:  35945861.283073574\n",
      "eval:  142310975.85160914\n",
      "eval:  562272959.5326883\n",
      "eval:  2227028768.1913137\n",
      "eval:  8839699080.901934\n",
      "eval:  35226079809.23506\n",
      "eval:  141250591686.22324\n",
      "eval:  572054090795.2092\n",
      "eval:  2352698167119.2905\n",
      "eval:  9884875252044.336\n",
      "eval:  42653326815277.65\n",
      "eval:  189665929804210.62\n",
      "eval:  869688764764284.0\n",
      "eval:  4103945917787582.5\n",
      "eval:  1.9852347391314376e+16\n",
      "eval:  9.798111783811626e+16\n",
      "eval:  4.911044571489379e+17\n",
      "eval:  2.489763581295076e+18\n",
      "eval:  1.2726430739447237e+19\n",
      "eval:  6.543056334586096e+19\n",
      "eval:  3.377803542105645e+20\n",
      "eval:  1.748825122341955e+21\n",
      "eval:  9.07313800183514e+21\n",
      "eval:  4.71434604736136e+22\n",
      "eval:  2.452269463561259e+23\n",
      "eval:  1.2766692426666322e+24\n",
      "eval:  6.650680496428408e+24\n",
      "eval:  3.466307561339118e+25\n",
      "eval:  1.8073056386894976e+26\n",
      "eval:  9.425823756045594e+26\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def evaluate(policy, values ,env):\n",
    "    gamma = 1\n",
    "    \n",
    "    nextValues = np.zeros(np.shape(values))\n",
    "        \n",
    "    for i in range(env.states[0]):\n",
    "        #print(i)\n",
    "        for j in range(env.states[1]):\n",
    "            for k in range(env.states[2]):\n",
    "                for l in range(env.states[3]):\n",
    "                    state = np.array([i,j,k,l])\n",
    "                    stateval = 0\n",
    "                    terminated, reward = env.find_terminated(state)\n",
    "\n",
    "                    if terminated:\n",
    "                        break\n",
    "                    for m in range(0,env.max_values[4]):\n",
    "                        \n",
    "                        stateNext = env.move_saw(state.copy(), env.actions[m])\n",
    "                        terminated, reward = env.find_terminated(stateNext)\n",
    "                        if terminated:\n",
    "                            stateNext = state\n",
    "                        \n",
    "                        stateval += policy[i][j][k][l][m] * (reward + gamma * values[stateNext[0]][stateNext[1]][stateNext[2]][stateNext[3]])\n",
    "                        \n",
    "                    if not terminated:\n",
    "                        nextValues[i][j][k][l] = stateval\n",
    "                        \n",
    "    return nextValues\n",
    "        \n",
    "\n",
    "        \n",
    "def iterate(policy, values, env):\n",
    "    \n",
    "    \n",
    "    \n",
    "    nextpolicy = np.zeros(np.shape(policy))\n",
    "    \n",
    "    for i in range(env.states[0]):\n",
    "        #print(i)\n",
    "        for j in range(env.states[1]):\n",
    "            for k in range(env.states[2]):\n",
    "                for l in range(env.states[3]):\n",
    "                    state = np.array([i,j,k,l])\n",
    "                    stateval = []\n",
    "                    terminated, reward = env.find_terminated(state)\n",
    "\n",
    "                    if terminated:\n",
    "                        break\n",
    "                    for m in range(0,env.max_values[4]):\n",
    "                        \n",
    "                        s = env.move_saw(state.copy(), env.actions[m])\n",
    "                        terminated, reward = env.find_terminated(s)\n",
    "                        if terminated:\n",
    "                            s = state.copy()\n",
    "                        \n",
    "                        stateval.append(values[s[0]][s[1]][s[2]][s[3]])\n",
    "                    #print(stateval)\n",
    "                        \n",
    "                    \n",
    "                    maxval = stateval[np.argmax(stateval)]\n",
    "                    #print(maxval)\n",
    "                    #print(\"duh\", state, maxval)\n",
    "                    #print(allNext)\n",
    "                    count = (stateval == maxval).sum()\n",
    "                    for m in range(0,env.max_values[4]):\n",
    "                        nextpolicy[i][j][k][l][m] = 1/count\n",
    "    return nextpolicy\n",
    "                        \n",
    "\n",
    "def knownGridworld():\n",
    "    env = SawWorldEnv(render_mode = \"rgb_array\")\n",
    "\n",
    "\n",
    "    states = env.states\n",
    "\n",
    "    values = np.zeros(states[0:4])\n",
    "\n",
    "    policy = np.ones(states) * 1/6\n",
    "    #nextvals = evaluate(policy, values, env)\n",
    "    \n",
    "    for y in range (10):\n",
    "        nextvals = np.zeros(states[0:4])\n",
    "        for x in range(100):\n",
    "            values = nextvals.copy()\n",
    "            #print(nextvals)\n",
    "            nextvals = evaluate(policy, values, env)\n",
    "            delta = np.sum(np.abs(nextvals - values))\n",
    "            print(\"eval: \",delta)\n",
    "            if delta <.001:\n",
    "                break\n",
    "        \n",
    "        newpolicy = iterate(policy, nextvals, env)\n",
    "        delpol = np.sum(np.abs(newpolicy - policy))\n",
    "        print(\"iter: \",delpol)\n",
    "        if delpol < .1:\n",
    "            break\n",
    "        policy = newpolicy\n",
    "    \n",
    "    env.close()\n",
    "    return newpolicy\n",
    "\n",
    "def test():\n",
    "    env = SawWorldEnv(render_mode = \"rgb_array\")\n",
    "\n",
    "\n",
    "    states = env.states\n",
    "\n",
    "    values = np.zeros(states[0:4])\n",
    "\n",
    "    policy = np.ones(states) * 1/6\n",
    "    nextvals = evaluate(policy, values, env)\n",
    "    newpolicy = iterate(policy, nextvals, env)\n",
    "knownGridworld()\n",
    "#test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bae07d184447cfc24ea699bd60b9e471feadbe0176d13e1965ad7b9fcf0dc8a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
